<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1">

<meta name="author" content="Jean Czerlinski Whitmore" />

<meta name="date" content="2019-08-19" />

<title>README</title>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css" data-origin="pandoc">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */

</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    for (var j = 0; j < rules.length; j++) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") continue;
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') continue;
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>



<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#header {
text-align: center;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; }  code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">README</h1>
<h4 class="author">Jean Czerlinski Whitmore</h4>
<h4 class="date">2019-08-19</h4>



<!-- Copy & paste & alter from ../README.Rmd
     * Changed header to produce vignette, not github markdown.
     * Removed installation info.
     * Added loading of heuristica.
-->
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="kw">library</span>(heuristica)</a></code></pre></div>
<p>The <code>heuristica</code> R package implements <a href="http://en.wikipedia.org/wiki/Heuristic">heuristic</a> decision models, such as <a href="http://en.wikipedia.org/wiki/Take-the-best_heuristic">Take The Best</a> (TTB) and a <a href="http://en.wikipedia.org/wiki/Unit-weighted_regression">unit-weighted linear model</a>. The models are designed for two-alternative choice tasks, such as which of two schools has a higher drop-out rate. The package also wraps more well-known models like regression and logistic regression into the two-alternative choice framework so all these models can be assessed side-by-side. It provides functions to measure accuracy, such as an overall <code>percentCorrect</code> and, for advanced users, some <a href="https://en.wikipedia.org/wiki/Confusion_matrix">confusion matrix</a> functions. These measures can be applied in-sample or out-of-sample.</p>
<p>The goal is to make it easy to explore the range of conditions in which simple heuristics are better than more complex models. Optimizing is not always better!</p>
<div id="the-task" class="section level1">
<h1>The Task</h1>
<p>This package is focused on two-alternative choice tasks, e.g. given two schools, which has a higher drop-out rate. The output is categorical, not quantitative.</p>
</div>
<div id="a-simple-example" class="section level1">
<h1>A Simple Example</h1>
<p>Here is a subset of data on Chicago public high school drop-out rates. The criterion to predict is the Dropout_Rate, which is in column 2.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2-1" data-line-number="1">schools &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">Name=</span><span class="kw">c</span>(<span class="st">&quot;Bowen&quot;</span>, <span class="st">&quot;Collins&quot;</span>, <span class="st">&quot;Fenger&quot;</span>, <span class="st">&quot;Juarez&quot;</span>, <span class="st">&quot;Young&quot;</span>), <span class="dt">Dropout_Rate=</span><span class="kw">c</span>(<span class="fl">25.5</span>, <span class="fl">11.8</span>, <span class="fl">28.7</span>, <span class="fl">21.6</span>, <span class="fl">4.5</span>), <span class="dt">Low_Income_Students=</span><span class="kw">c</span>(<span class="fl">82.5</span>, <span class="fl">88.8</span>, <span class="fl">63.2</span>, <span class="fl">84.5</span>, <span class="fl">30.3</span>), <span class="dt">Limited_English_Students=</span><span class="kw">c</span>(<span class="fl">11.4</span>, <span class="fl">0.1</span>, <span class="dv">0</span>, <span class="fl">28.3</span>, <span class="fl">0.1</span>))</a>
<a class="sourceLine" id="cb2-2" data-line-number="2">schools</a></code></pre></div>
<pre><code>##      Name Dropout_Rate Low_Income_Students Limited_English_Students
## 1   Bowen         25.5                82.5                     11.4
## 2 Collins         11.8                88.8                      0.1
## 3  Fenger         28.7                63.2                      0.0
## 4  Juarez         21.6                84.5                     28.3
## 5   Young          4.5                30.3                      0.1</code></pre>
<div id="fitting" class="section level2">
<h2>Fitting</h2>
<p>To fit a model, we give it the data set and the columns to use. In this case, the 2nd column, <code>Dropout_Rate</code>, is the <strong>criterion</strong> to be predicted. The <strong>cues</strong> are the following columns, percent of <code>Low_Income_Students</code> and percent of <code>Limited_English_Students</code>. They are at indexes 3 and 4.</p>
<p>Let’s fit two models: * ttbModel, Take The Best, which uses the highest-validity cue that discriminates (more details below). * regModel, a version of R’s “lm” function for linear regression wrapped to fit into heurstica’s interface.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb4-1" data-line-number="1">criterion_col &lt;-<span class="st"> </span><span class="dv">2</span></a>
<a class="sourceLine" id="cb4-2" data-line-number="2">ttb &lt;-<span class="st"> </span><span class="kw">ttbModel</span>(schools, criterion_col, <span class="kw">c</span>(<span class="dv">3</span><span class="op">:</span><span class="dv">4</span>))</a>
<a class="sourceLine" id="cb4-3" data-line-number="3">reg &lt;-<span class="st"> </span><span class="kw">regModel</span>(schools, criterion_col, <span class="kw">c</span>(<span class="dv">3</span><span class="op">:</span><span class="dv">4</span>))</a></code></pre></div>
<p>What do the fits look like? We can examine Take The Best’s cue validities and the regression coefficients.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb5-1" data-line-number="1">ttb<span class="op">$</span>cue_validities</a></code></pre></div>
<pre><code>##      Low_Income_Students Limited_English_Students 
##                0.6000000                0.5555556</code></pre>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb7-1" data-line-number="1"><span class="kw">coef</span>(reg)</a></code></pre></div>
<pre><code>##      Low_Income_Students Limited_English_Students 
##               0.24985315               0.07322294</code></pre>
<p>Both Take The Best and regression give a higher weight to <code>Low_Income_Students</code> than <code>Limited_English_Students</code>, although of course how they use the weights differs. Take The Best will use a lexicographic order, making its prediction based solely on <code>Low_Income_Students</code> as long as the schools have differing values– which they do for all 5 schools in this data set. That means it will ignore <code>Limited_English_Students</code> when predicting on this data set. In contrast, regression will use a weighted sum of both cues, but with the most important cues weighted more.</p>
</div>
<div id="predicting-the-fitted-data" class="section level2">
<h2>Predicting the fitted data</h2>
<p>To see a model’s predictions, we use the <code>predictPair</code> function. It takes two rows of data– which together comprise a “row pair”– and the fitted model. <code>predictPair</code> outputs three possible values:</p>
<ul>
<li>1 predicts the first row passed to it.</li>
<li>-1 predicts the second row passed to it.</li>
<li>0 is a guess.</li>
</ul>
<p>In Bowen vs. Collins, it outputs 1, meaning it predicts Bowen has a higher dropout rate. In Bowen vs. Fenger, it outputs -1, meaning it predicts Fenger has a higher dropout rate.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb9-1" data-line-number="1"><span class="kw">predictPair</span>(<span class="kw">subset</span>(schools, Name<span class="op">==</span><span class="st">&quot;Bowen&quot;</span>), <span class="kw">subset</span>(schools, Name<span class="op">==</span><span class="st">&quot;Collins&quot;</span>), ttb)</a></code></pre></div>
<pre><code>## [1] 1</code></pre>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb11-1" data-line-number="1"><span class="kw">predictPair</span>(<span class="kw">subset</span>(schools, Name<span class="op">==</span><span class="st">&quot;Bowen&quot;</span>), <span class="kw">subset</span>(schools, Name<span class="op">==</span><span class="st">&quot;Fenger&quot;</span>), ttb)</a></code></pre></div>
<pre><code>## [1] -1</code></pre>
<p>Note that the output depends on the order of the rows. In the reversed pair of Collins vs. Bowen, the output is -1. This is consistent because it still picks Bowen, regardless of order.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb13-1" data-line-number="1"><span class="kw">predictPair</span>(<span class="kw">subset</span>(schools, Name<span class="op">==</span><span class="st">&quot;Collins&quot;</span>), <span class="kw">subset</span>(schools, Name<span class="op">==</span><span class="st">&quot;Bowen&quot;</span>), ttb)</a></code></pre></div>
<pre><code>## [1] -1</code></pre>
</div>
<div id="all-rows" class="section level2">
<h2>All rows</h2>
<p>It is tedious to predict one row pair at a time, so let’s use heurstica’s <code>predictPairSummary</code> function instead. We simply pass it the data and the heuristics whose predictions we are interested in. It produces a matrix with all row pairs, which in this case is 10 (5 * 4 / 2).</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb15-1" data-line-number="1">out &lt;-<span class="st"> </span><span class="kw">predictPairSummary</span>(schools, ttb, reg)</a>
<a class="sourceLine" id="cb15-2" data-line-number="2"><span class="co"># See the first row: It has row indexes.</span></a>
<a class="sourceLine" id="cb15-3" data-line-number="3">out[<span class="dv">1</span>,]</a></code></pre></div>
<pre><code>##           Row1           Row2 CorrectGreater       ttbModel       regModel 
##              1              2              1              1             -1</code></pre>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb17-1" data-line-number="1"><span class="co"># Convert indexes to school names for easier interpretation</span></a>
<a class="sourceLine" id="cb17-2" data-line-number="2">out_df &lt;-<span class="st"> </span><span class="kw">data.frame</span>(out)</a>
<a class="sourceLine" id="cb17-3" data-line-number="3">out_df<span class="op">$</span>Row1 &lt;-<span class="st"> </span>schools<span class="op">$</span>Name[out_df<span class="op">$</span>Row1]</a>
<a class="sourceLine" id="cb17-4" data-line-number="4">out_df<span class="op">$</span>Row2 &lt;-<span class="st"> </span>schools<span class="op">$</span>Name[out_df<span class="op">$</span>Row2]</a>
<a class="sourceLine" id="cb17-5" data-line-number="5">out_df</a></code></pre></div>
<pre><code>##       Row1    Row2 CorrectGreater ttbModel regModel
## 1    Bowen Collins              1        1       -1
## 2    Bowen  Fenger             -1       -1        1
## 3    Bowen  Juarez              1        1       -1
## 4    Bowen   Young              1       -1        1
## 5  Collins  Fenger             -1       -1        1
## 6  Collins  Juarez             -1       -1       -1
## 7  Collins   Young              1       -1        1
## 8   Fenger  Juarez              1        1       -1
## 9   Fenger   Young              1       -1        1
## 10  Juarez   Young              1       -1        1</code></pre>
<p>The first row shows the Bowen vs. Collins example we considered above. Because CorrectGreater is 1, that means TTB predicted it correctly– Bowen really does have a higher drop-out rate. But regression predicted -1 for this row pair, which is incorrect.</p>
<p>predictPairSummary is for beginners. heuristica offers full flexibility in output with the <code>rowPairApply</code> function. After passing it the data, you can pass it any number of generators to make the columns you want. Some examples are below, where we print only the first row.</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb19-1" data-line-number="1"><span class="co"># Same as predictPairSummary.</span></a>
<a class="sourceLine" id="cb19-2" data-line-number="2">out_same &lt;-<span class="st"> </span><span class="kw">rowPairApply</span>(schools, <span class="kw">rowIndexes</span>(), <span class="kw">correctGreater</span>(criterion_col), <span class="kw">heuristics</span>(ttb, reg))</a>
<a class="sourceLine" id="cb19-3" data-line-number="3">out_same[<span class="dv">1</span>,]</a></code></pre></div>
<pre><code>##           Row1           Row2 CorrectGreater       ttbModel       regModel 
##              1              2              1              1             -1</code></pre>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb21-1" data-line-number="1"><span class="co"># Show first the heuristic predictions, then CorrectGreater.  No row indexes.</span></a>
<a class="sourceLine" id="cb21-2" data-line-number="2">out_simple &lt;-<span class="st"> </span><span class="kw">rowPairApply</span>(schools, <span class="kw">heuristics</span>(ttb, reg), <span class="kw">correctGreater</span>(criterion_col))</a>
<a class="sourceLine" id="cb21-3" data-line-number="3">out_simple[<span class="dv">1</span>,]</a></code></pre></div>
<pre><code>##       ttbModel       regModel CorrectGreater 
##              1             -1              1</code></pre>
</div>
<div id="assessing-overall-performance" class="section level2">
<h2>Assessing Overall Performance</h2>
<p>For an overall measure of performance, we can measure the percent of correct inferences for all pairs of schools in the data with <code>percentCorrect</code>, namely the number of correct predictions divided by the total number of predictions. We give the function the data to be predicted (in this case the same as what was fit) and the fitted models to assess.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb23-1" data-line-number="1"><span class="kw">percentCorrect</span>(schools, ttb, reg)</a></code></pre></div>
<pre><code>##   ttbModel regModel
## 1       60       50</code></pre>
<p>Take The Best got 60% correct and regression got 50% correct, which is the same as chance.</p>
<p>Regression is the best linear unbiased model for the data. But this data had a very small sample size of just 5 schools, and good estimates require more data.</p>
<p>This is an unusual case where TTB actually beat regression in a fitting task. Usually ttb only wins in out-of-sample performance, e.g. fitting 5 schools and then predicting on other schools not used in the fit.</p>
<p>For a more realistic example, see the vignette with cross-validated out-of-sample performance on a complete data set.</p>
</div>
</div>
<div id="models" class="section level1">
<h1>Models</h1>
<p>The package comes with the following models that you can call with predictPair.</p>
<ul>
<li><code>logRegModel</code>: A logistic regression model, a wrapper around R’s glm. This estimates the probability that one school’s drop-out rate is greater than the other and chooses the school with probability greater than 50%.</li>
<li><code>minModel</code>: It searches cues in a random order, making a decision based on the first cue that discriminates (has differing values on the two items / schools).</li>
<li><code>regModel</code>: A regression model, a wrapper around R’s lm to make it easier to compare with heuristics. It fits a regression based on the column indices. For predictPair, it predicts the criterion for each item in the pair– e.g. estimates the drop-out rate of each school– and then predicts the item with the higher estimate– higher drop-out rate. (A variant that fits with an intercept, <code>regInterceptModel</code>, is provided in order to confirm prior research results, but it is not recommended for future research.)</li>
<li><code>singleCueModel</code>: In the fitting stage, this selects the cue with the higest cue validity. It only uses that cue, and if the cue does not discriminate, it guesses.</li>
<li><code>ttbModel</code>: An implementation of <a href="http://en.wikipedia.org/wiki/Take-the-best_heuristic">Take The Best</a>. In the fitting stage, it sorts cues in order of cue validity. When predicting between two items, it finds the highest-validity that discriminates (has differing values on the two items) and bases its prediction on that cue, ignoring other cues. The cue used can vary based on the cue values of the two items.</li>
<li><code>ttbGreedyModel</code>: Take the Best using conditional cue validity (rather than cue validity).</li>
<li><code>unitWeightModel</code>: A <a href="http://en.wikipedia.org/wiki/Unit-weighted_regression">unit-weighted linear model</a> that uses weights of +1 or -1 only. An exception is that a cue with no variance– every value is the same– gets a weight of 0. Inspired by psychologist Robyn Dawes– see citation below.</li>
<li><code>validityWeightModel</code>: A cue-validity-weighted linear model. (In some publications, this was called franklinModel after Ben Franklin.)</li>
</ul>
<p>You can add your own models by also implementing a function related to <code>predictPair</code>, as described in a vignette.</p>
</div>
<div id="data" class="section level1">
<h1>Data</h1>
<p>The package comes with two data sets used by many heuristic researchers.</p>
<ul>
<li><code>city_population</code>: The 83 German cities with more than 100,000 inhabitants in 1993. All cues are binary. (There is another version called <code>city_population_original</code> that has some transciption errors from the almanac source but exactly matches the data set in Simple Heuristics That Make Us Smart.)</li>
<li><code>highschool_dropout</code>: Drop-out rates for all 63 Chicago public high schools plus associated variables like average students per teacher and percent low income students. The data is from 1995. All cues are real-valued but some have N/A values. (This data set does not exactly match that used in Simple Heuristics That Make Us Smart.)</li>
</ul>
</div>
<div id="citations" class="section level1">
<h1>Citations</h1>
<p>Take The Best was first described in: Gigerenzer, G. &amp; Goldstein, D. G. (1996). “Reasoning the fast and frugal way: Models of bounded rationality”. Psychological Review, 103, 650-669.</p>
<p>All of these heuristics were run on many data sets and analyzed in: Gigerenzer, G., Todd, P. M., &amp; the ABC Group (1999). <a href="http://www.amazon.com/Simple-Heuristics-That-Make-Smart/dp/0195143817">Simple heuristics that make us smart.</a> New York: Oxford University Press.</p>
<p>The research was also inspired by: Dawes, Robyn M. (1979). “The robust beauty of improper linear models in decision making”. American Psychologist, volume 34, pages 571-582. <a href="http://www.cmu.edu/dietrich/sds/docs/dawes/the-robust-beauty-of-improper-linear-models-in-decision-making.pdf">archived pdf</a></p>
</div>
<div id="acknowledgements" class="section level1">
<h1>Acknowledgements</h1>
<p>Thanks for coding advice and beta testing go to Marcus Buckmann, Daniel G. Goldstein, and Özgür Simsek.</p>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
